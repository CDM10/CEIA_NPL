{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEnBiuLcukJc"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## RNN one-to-one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i96B2RF8uqEb"
   },
   "source": [
    "#### Datos\n",
    "El objecto es utilizar una serie de sucuencias númericas (datos sintéticos) para poner a prueba el uso de las redes RNN. Este ejemplo se inspiró en otro artículo, lo tienen como referencia en el siguiente link:\\\n",
    "[LINK](https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lx0HQ-1RvJw9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM, SimpleRNN\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10bFkG1YuaD9"
   },
   "outputs": [],
   "source": [
    "# Generar datos sintéticos\n",
    "X = list()\n",
    "y = list()\n",
    "X = [x+1 for x in range(20)]\n",
    "\n",
    "# \"y\" (target) se obtiene como cada dato de entrada multiplicado por 15\n",
    "y = [x * 15 for x in X]\n",
    "\n",
    "print(\"datos X:\", X)\n",
    "print(\"datos y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oqabd-kYvza9"
   },
   "outputs": [],
   "source": [
    "# Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1)\n",
    "X = np.array(X).reshape(len(X), 1, 1)\n",
    "print(\"datos X:\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYz6XpuyxBbQ"
   },
   "outputs": [],
   "source": [
    "y = np.asanyarray(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG3-d_NXwDGD"
   },
   "source": [
    "### 2 - Entrenar el modelo (RNN y LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFeZEc63wOvJ"
   },
   "outputs": [],
   "source": [
    "input_shape = X[0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZir-NqDwWEo"
   },
   "outputs": [],
   "source": [
    "output_shape = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAhw8O9mwLR0"
   },
   "outputs": [],
   "source": [
    "# Comenzamos con una RNN clásica\n",
    "# En general una celda RNN clásica ya no se utiliza, es solo a modo de ejemplo\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64, activation='relu', input_shape=input_shape))\n",
    "model.add(Dense(output_shape))\n",
    "model.compile(loss='mse',\n",
    "              optimizer=\"Adam\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSX93pkow2zM"
   },
   "outputs": [],
   "source": [
    "hist = model.fit(X, y, epochs=500, validation_split=0.2, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anuBmCv0xNGA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Entrenamiento\n",
    "epoch_count = range(1, len(hist.history['loss']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=hist.history['loss'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=hist.history['val_loss'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88tdVCOyxcuy"
   },
   "outputs": [],
   "source": [
    "# Ensayo\n",
    "# x = 30\n",
    "# y_test = x * 15\n",
    "\n",
    "x_test = 30\n",
    "y_test = x_test * 15\n",
    "test_input = np.array([x_test])\n",
    "test_input = test_input.reshape((1, 1, 1))\n",
    "y_hat = model.predict(test_input, verbose=0)[0][0]\n",
    "\n",
    "print(\"y_test:\", y_test)\n",
    "print(\"y_hat:\", y_hat)\n",
    "\n",
    "model.evaluate(test_input, np.array([y_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0-pi5qmVxav"
   },
   "outputs": [],
   "source": [
    "# Ahora probaremos con LSTM, qué es más compleja y por lo tanto\n",
    "# requiere más parámetros a entrenar\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(64, activation='relu', input_shape=input_shape))\n",
    "model2.add(Dense(output_shape))\n",
    "model2.compile(loss='mse',\n",
    "              optimizer=\"Adam\")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bp5r9gUEWLVf"
   },
   "outputs": [],
   "source": [
    "hist2 = model2.fit(X, y, epochs=500, validation_split=0.2, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNyQ__9fWUPC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Entrenamiento\n",
    "epoch_count = range(1, len(hist2.history['loss']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=hist2.history['loss'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=hist2.history['val_loss'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuCV-UBhWNb3"
   },
   "outputs": [],
   "source": [
    "# Ensayo\n",
    "# x = 30\n",
    "# y_test = x * 15\n",
    "\n",
    "x_test = 30\n",
    "y_test = x_test * 15\n",
    "test_input = np.array([x_test])\n",
    "test_input = test_input.reshape((1, 1, 1))\n",
    "y_hat = model2.predict(test_input, verbose=0)[0][0]\n",
    "\n",
    "print(\"y_test:\", y_test)\n",
    "print(\"y_hat:\", y_hat)\n",
    "\n",
    "model2.evaluate(test_input, np.array([y_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEI5TjSFWeY8"
   },
   "source": [
    "Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT8b9EfGyshD"
   },
   "source": [
    "### 3 - Multi-layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cH8Yd6WYyzQQ"
   },
   "outputs": [],
   "source": [
    "# En esta oportunidad se utilizarán dos layer LSTM. Para poder conectar\n",
    "# la primera layer con la segunda se debe colocar return_sequences=True\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(64, activation='relu', return_sequences=True, input_shape=input_shape))\n",
    "model3.add(LSTM(64, activation='relu'))\n",
    "model3.add(Dense(output_shape))\n",
    "model3.compile(loss='mse',\n",
    "              optimizer=\"Adam\")\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLtlpYpxzQZr"
   },
   "outputs": [],
   "source": [
    "hist3 = model3.fit(X, y, epochs=500, validation_split=0.2, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3Sl3cUJzZV_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Entrenamiento\n",
    "epoch_count = range(1, len(hist3.history['loss']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=hist3.history['loss'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=hist3.history['val_loss'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FveVOv2xzfkC"
   },
   "outputs": [],
   "source": [
    "# Ensayo\n",
    "# x = 30\n",
    "# y_test = x * 15\n",
    "\n",
    "x_test = 30\n",
    "y_test = x_test * 15\n",
    "test_input = np.array([x_test])\n",
    "test_input = test_input.reshape((1, 1, 1))\n",
    "y_hat = model3.predict(test_input, verbose=0)[0][0]\n",
    "\n",
    "print(\"y_test:\", y_test)\n",
    "print(\"y_hat:\", y_hat)\n",
    "\n",
    "model3.evaluate(test_input, np.array([y_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd1g5MZfz5qB"
   },
   "source": [
    "### 4 - Conclusión\n",
    "Implementar un modelo basado en RNN o LSTM es muy sensillo, hay que tener en cuenta que al apilar varias layers hay que colocar el flag \"return_sequence\" en \"True\".\n",
    "El resultado alcanzado es bueno pero podría mejorarse agregando más layer LSTM o más layer Densas"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNtWEL/wkSrAU+5MYlOJ7sj",
   "collapsed_sections": [],
   "name": "4a - one-to-one.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
