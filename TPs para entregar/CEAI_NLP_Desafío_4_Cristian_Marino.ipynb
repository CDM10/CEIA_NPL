{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TRABAJO PRÁCTICO 4 - NLP - Bot QA\n",
        "\n",
        "Construir QA Bot basado en el ejemplo del traductor pero con un dataset QA.\n",
        "\n",
        "Tener en cuenta las siguientes recomendaciones:\n",
        "\n",
        "1. MAX_VOCAB_SIZE = 8000\n",
        "2. max_length ~ 10\n",
        "3. Embeddings 300 Fasttext\n",
        "4. n_units = 128\n",
        "5. LSTM Dropout 0.2\n",
        "6. Epochs 30~50\n",
        "\n",
        "Preguntas interesantes:\n",
        "1. Do you read?\n",
        "2. Do you have any pet?\n",
        "3. Where are you from?"
      ],
      "metadata": {
        "id": "hNLHekTFYYzu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UNoy2HapRPI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM, SimpleRNN\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input"
      ],
      "metadata": {
        "id": "p_qfdFWTY_2F"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar la carpeta de dataset\n",
        "import os\n",
        "import gdown\n",
        "if os.access('data_volunteers.json', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n",
        "    output = 'data_volunteers.json'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlWiuJJ7ZSoE",
        "outputId": "cc5686fd-a8bd-4bea-ed70-361b7e4a17c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download\n",
            "To: /content/data_volunteers.json\n",
            "100%|██████████| 2.58M/2.58M [00:00<00:00, 164MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_file\n",
        "import json\n",
        "\n",
        "text_file = \"data_volunteers.json\"\n",
        "with open(text_file) as f:\n",
        "    data = json.load(f) # la variable data será un diccionario"
      ],
      "metadata": {
        "id": "eubAktQJZ_rk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Observar los campos disponibles en cada linea del dataset\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP9BeUG2aD_M",
        "outputId": "246b9063-d0a7-4496-efe8-d6b927fe17c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_in = []\n",
        "chat_out = []\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "max_len = 30\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt.replace(\"\\'d\", \" had\")\n",
        "    txt.replace(\"\\'s\", \" is\")\n",
        "    txt.replace(\"\\'m\", \" am\")\n",
        "    txt.replace(\"don't\", \"do not\")\n",
        "    txt = re.sub(r'\\W+', ' ', txt)\n",
        "\n",
        "    return txt\n",
        "\n",
        "for line in data:\n",
        "    for i in range(len(line['dialog'])-1):\n",
        "        # vamos separando el texto en \"preguntas\" (chat_in)\n",
        "        # y \"respuestas\" (chat_out)\n",
        "        chat_in = clean_text(line['dialog'][i]['text'])\n",
        "        chat_out = clean_text(line['dialog'][i+1]['text'])\n",
        "\n",
        "        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n",
        "            continue\n",
        "\n",
        "        input_sentence, output = chat_in, chat_out\n",
        "\n",
        "        # output sentence (decoder_output) tiene <eos>\n",
        "        output_sentence = output + ' <eos>'\n",
        "        # output sentence input (decoder_input) tiene <sos>\n",
        "        output_sentence_input = '<sos> ' + output\n",
        "\n",
        "        input_sentences.append(input_sentence)\n",
        "        output_sentences.append(output_sentence)\n",
        "        output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "print(\"Cantidad de rows utilizadas:\", len(input_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBmHE9FnaICc",
        "outputId": "6cff3426-0919-468c-c466-585125ad5dc6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de rows utilizadas: 6033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences[1], output_sentences[1], output_sentences_inputs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53VTG3_XaN0U",
        "outputId": "349fb1bf-74cd-4b29-afe5-f7b5dfc244fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hi how are you ', 'not bad and you  <eos>', '<sos> not bad and you ')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB_SIZE = 8000"
      ],
      "metadata": {
        "id": "A1uk0L5zR7LN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizar las oraciones de entrada\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer_inputs.fit_on_texts(input_sentences)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_sentences)\n",
        "\n",
        "# Longitud máxima de las secuencias de entrada\n",
        "max_input_len = 10\n",
        "\n",
        "# Crear word2idx para las entradas\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "num_words_input = len(word2idx_inputs) + 1\n",
        "\n",
        "# Tokenizar las oraciones de salida\n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer_outputs.fit_on_texts(output_sentences + output_sentences_inputs)\n",
        "output_sequences = tokenizer_outputs.texts_to_sequences(output_sentences)\n",
        "output_sequences_inputs = tokenizer_outputs.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "# Longitud máxima de las secuencias de salida\n",
        "max_output_len = 10\n",
        "\n",
        "# Añadir tokens especiales a las oraciones de salida durante la tokenización\n",
        "tokenizer_outputs = Tokenizer(filters='')\n",
        "tokenizer_outputs.fit_on_texts(output_sentences + output_sentences_inputs)\n",
        "output_sequences = tokenizer_outputs.texts_to_sequences(output_sentences)\n",
        "output_sequences_inputs = tokenizer_outputs.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "# Crear word2idx para las salidas\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "word2idx_outputs['<sos>'] = tokenizer_outputs.word_index.get('<sos>', len(word2idx_outputs) + 1)\n",
        "word2idx_outputs['<eos>'] = tokenizer_outputs.word_index.get('<eos>', len(word2idx_outputs) + 1)\n",
        "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE)\n",
        "\n",
        "# Padding de las secuencias\n",
        "encoder_input_sequences = pad_sequences(input_sequences, maxlen=max_input_len)\n",
        "decoder_input_sequences = pad_sequences(output_sequences_inputs, maxlen=max_output_len)\n",
        "decoder_output_sequences = pad_sequences(output_sequences, maxlen=max_output_len, padding='post')"
      ],
      "metadata": {
        "id": "iv3KKXsotGwL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cantidad de rows del dataset:\", len(input_sequences))\n",
        "print(\"encoder_input_sequences shape:\", encoder_input_sequences.shape)\n",
        "print(\"decoder_input_sequences shape:\", decoder_input_sequences.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfdgHi-sUtXw",
        "outputId": "552bb8e1-eead-4cd6-c9f2-5e1dd3a60ce4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de rows del dataset: 6033\n",
            "encoder_input_sequences shape: (6033, 10)\n",
            "decoder_input_sequences shape: (6033, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "decoder_targets = to_categorical(decoder_output_sequences, num_classes=num_words_output)\n",
        "decoder_targets.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvqfCrllVPy0",
        "outputId": "0db0e9fa-bb79-4d9a-8d6f-dca70b103aa1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6033, 10, 1807)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_sequences.shape, decoder_input_sequences.shape, decoder_output_sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAvf9mxQFs-I",
        "outputId": "3f4b6b25-2b3b-40c5-c7e6-b453624b3ef4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6033, 10), (6033, 10), (6033, 10, 1, 1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBj3kgaDza3a",
        "outputId": "6715f1ae-173c-4e7c-f51d-b4a0d9307762"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ruta_archivo = '/content/drive/My Drive/NLP CEIA/glove_300d.txt'\n",
        "embedding_dim = 300\n",
        "\n",
        "# Cargar embeddings preentrenados (GloVe de 300 dimensiones)\n",
        "embeddings_index = {}\n",
        "with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "metadata": {
        "id": "tp2My13w1LRs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear matriz de embedding para entradas\n",
        "embedding_matrix_input = np.zeros((num_words_input, embedding_dim))\n",
        "for word, i in word2idx_inputs.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_input[i] = embedding_vector\n",
        "\n",
        "# Crear matriz de embedding para salidas\n",
        "embedding_matrix_output = np.zeros((num_words_output, embedding_dim))\n",
        "for word, i in word2idx_outputs.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_output[i] = embedding_vector"
      ],
      "metadata": {
        "id": "wTnDg-BR17Ht"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo encoder\n",
        "encoder_inputs = Input(shape=(10,))\n",
        "encoder_embedding = Embedding(num_words_input,embedding_dim , weights=[embedding_matrix_input], trainable=False)(encoder_inputs)\n",
        "encoder_lstm = LSTM(128, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Crear el modelo decoder\n",
        "decoder_inputs = Input(shape=(10,))\n",
        "decoder_embedding = Embedding(num_words_output, embedding_dim, weights=[embedding_matrix_output], trainable=False)(decoder_inputs)\n",
        "decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Definir el modelo\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "IpC4MrNg190E"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoCnZYRJ3QYV",
        "outputId": "ac9b92f4-2461-4359-9cf8-40ffe3347b9a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 10, 300)      540000      ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 10, 300)      542100      ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 128),        219648      ['embedding_2[0][0]']            \n",
            "                                 (None, 128),                                                     \n",
            "                                 (None, 128)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, 10, 128),    219648      ['embedding_3[0][0]',            \n",
            "                                 (None, 128),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 128)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 10, 1807)     233103      ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,754,499\n",
            "Trainable params: 672,399\n",
            "Non-trainable params: 1,082,100\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir las secuencias de salida en una forma compatible para entrenamiento\n",
        "decoder_output_sequences = np.squeeze(decoder_output_sequences)\n",
        "decoder_output_sequences.shape\n",
        "# Entrenar el modelo (Lo entrené dos veces con 30 epochs)\n",
        "#model_fit=model.fit([encoder_input_sequences, decoder_input_sequences], decoder_targets, batch_size=32, epochs=30, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syAA_6sI2l2K",
        "outputId": "5c66028d-d568-4253-89ca-5f0ee1d7e09b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6033, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar si hay GPU disponible. NO TENGO GPU DISPONIBLE Y ME GASTÓ LAS UI :'(\n",
        "gpu_available = tf.config.list_physical_devices('GPU')\n",
        "if gpu_available:\n",
        "    print(f\"GPU disponible: {gpu_available}\")\n",
        "else:\n",
        "    print(\"No hay GPU disponible.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukpv2RXc4hOt",
        "outputId": "0432e8d2-9f02-4caa-bbd8-058f464ed230"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No hay GPU disponible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3nX8-4T8F-Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Forma de encoder_input:\", encoder_inputs.shape)  # Debe ser (n_samples, max_input_len)\n",
        "print(\"Forma de decoder_input:\", decoder_inputs.shape)  # Debe ser (n_samples, max_output_len)\n",
        "print(\"Forma de decoder_output:\", decoder_outputs.shape)  # Debe ser (n_samples, 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZlJmQDK7iCN",
        "outputId": "fa22ae46-6696-454a-9a96-95cc0d198988"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma de encoder_input: (None, 10)\n",
            "Forma de decoder_input: (None, 10)\n",
            "Forma de decoder_output: (None, 10, 1807)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo encoder para inferencia\n",
        "encoder_model_inf = model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Modelo decoder para inferencia\n",
        "decoder_state_input_h = Input(shape=(128,))\n",
        "decoder_state_input_c = Input(shape=(128,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model_inf = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model_inf.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model_inf.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = tokenizer_outputs.index_word.get(sampled_token_index, '')\n",
        "\n",
        "        if sampled_word == '<eos>' or len(decoded_sentence) > max_output_len:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    # Limpiar la oración de entrada\n",
        "    sentence = clean_text(sentence)\n",
        "    # Convertir la oración en una secuencia de índices\n",
        "    sequence = tokenizer_inputs.texts_to_sequences([sentence])\n",
        "    # Hacer padding a la secuencia para que tenga la longitud máxima de entrada\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_input_len)\n",
        "    return padded_sequence\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Obtener los estados internos del encoder\n",
        "    states_value = encoder_model_inf.predict(input_seq)\n",
        "\n",
        "    # Generar una secuencia vacía de longitud 1 con el índice del token de inicio\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "    decoded_sentence = ''\n",
        "    stop_condition = False\n",
        "    while not stop_condition:\n",
        "        # Obtener las predicciones del decoder\n",
        "        output_tokens, h, c = decoder_model_inf.predict([target_seq] + states_value)\n",
        "\n",
        "        # Obtener el índice del token con mayor probabilidad\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = tokenizer_outputs.index_word.get(sampled_token_index, '')\n",
        "\n",
        "        if sampled_word == '<eos>' or len(decoded_sentence.split()) > max_output_len:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # Actualizar la secuencia objetivo (target_seq)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Actualizar los estados del decoder\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "j3NMvwRI45xL",
        "outputId": "bf4af41c-bf85-4511-cf13-01fc96005e49"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Exception encountered when calling layer \"model_1\" (type Functional).\n\nCould not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 10, 1807), dtype=tf.float32, name=None), name='dense_1/Softmax:0', description=\"created by layer 'dense_1'\")\n\nCall arguments received by layer \"model_1\" (type Functional):\n  • inputs=tf.Tensor(shape=(None, 10), dtype=float32)\n  • training=['tf.Tensor(shape=(None, 128), dtype=float32)', 'tf.Tensor(shape=(None, 128), dtype=float32)']\n  • mask=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-bcde3b9f8999>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Modelo encoder para inferencia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoder_model_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Modelo decoder para inferencia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdecoder_state_input_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mx_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mx_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Could not compute output \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m             \u001b[0moutput_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Exception encountered when calling layer \"model_1\" (type Functional).\n\nCould not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 10, 1807), dtype=tf.float32, name=None), name='dense_1/Softmax:0', description=\"created by layer 'dense_1'\")\n\nCall arguments received by layer \"model_1\" (type Functional):\n  • inputs=tf.Tensor(shape=(None, 10), dtype=float32)\n  • training=['tf.Tensor(shape=(None, 128), dtype=float32)', 'tf.Tensor(shape=(None, 128), dtype=float32)']\n  • mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar el modelo\n",
        "input_sentence = \"Where are you from\"\n",
        "preprocessed_sentence = preprocess_sentence(input_sentence)\n",
        "decoded_sentence = decode_sequence(preprocessed_sentence)\n",
        "print(f'Q: {input_sentence}')\n",
        "print(f'A: {decoded_sentence}')"
      ],
      "metadata": {
        "id": "thlsgn_k5Tgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "ABBl6KNyv9m6"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}